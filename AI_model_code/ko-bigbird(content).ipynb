{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u4005/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-05 15:46:52.215712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-05 15:46:52.714604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ===============================\n",
    "# 0. ê¸´ í…ìŠ¤íŠ¸ chunk í•¨ìˆ˜ (ë¬¸ì ê¸°ì¤€)\n",
    "# ===============================\n",
    "\n",
    "def split_into_chunks(text, chunk_chars=4000):\n",
    "    \"\"\"\n",
    "    ê¸´ ë¬¸ìì—´ì„ chunk_chars ê¸€ì ë‹¨ìœ„ë¡œ ì˜ë¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.\n",
    "    ex) chunk_chars=4000ì´ë©´ 0~3999, 4000~7999, ... ì´ëŸ° ì‹ìœ¼ë¡œ ë‚˜ë‰¨.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    return [text[i:i + chunk_chars] for i in range(0, len(text), chunk_chars)]\n",
    "\n",
    "# ===============================\n",
    "# 1. Dataset ì •ì˜\n",
    "# ===============================\n",
    "\n",
    "class NewsLongformerDataset(Dataset):\n",
    "    def __init__(self, df, referrers, tokenizer, max_length=4096):\n",
    "        \"\"\"\n",
    "        df: index = article_id, columns = ['content', <referrer1> ...]\n",
    "        referrers: ë¼ë²¨ë¡œ ì“¸ referrer ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index()\n",
    "        self.referrers = referrers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row[\"content\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Longformer: ì²« í† í° global attention\n",
    "        global_attention_mask = torch.zeros_like(attention_mask)\n",
    "        global_attention_mask[0] = 1\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            [row[r] for r in self.referrers],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"global_attention_mask\": global_attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Soft Labelìš© Trainer\n",
    "# ===============================\n",
    "\n",
    "class SoftLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # labels: soft label (í™•ë¥  ë¶„í¬)\n",
    "        labels = inputs.pop(\"labels\")  # (B, num_labels)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits                    # (B, num_labels)\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # soft label cross-entropy: - sum_k y_k log p_k\n",
    "        loss = -(labels * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. metrics ê³„ì‚° í•¨ìˆ˜\n",
    "# ===============================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Trainerê°€ eval ì‹œ í˜¸ì¶œ.\n",
    "    predictions: logits (numpy)\n",
    "    label_ids: soft label ë¶„í¬ (numpy)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    # soft labelì—ì„œ argmaxë¡œ \"ì •ë‹µ referrer\" ì¸ë±ìŠ¤\n",
    "    # argmax ë°˜í™˜ ê°’ì€ class ë²ˆí˜¸ë§Œ ë‚˜ì˜´\n",
    "    true_idx = labels.argmax(axis=-1)\n",
    "    pred_idx = probs.argmax(axis=-1)\n",
    "    accuracy = (true_idx == pred_idx).mean()\n",
    "\n",
    "    # 1) Cross-Entropy (soft label ê¸°ì¤€)\n",
    "    eps = 1e-12\n",
    "    ce = -(labels * np.log(probs + eps)).sum(axis=-1).mean()\n",
    "\n",
    "    # 2) MSE / Brier score\n",
    "    mse = ((probs - labels) ** 2).mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"cross_entropy\": float(ce),\n",
    "        \"mse\": float(mse),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. ë°ì´í„° ì¤€ë¹„ + í•™ìŠµ í•¨ìˆ˜\n",
    "# ===============================\n",
    "\n",
    "def train_model(\n",
    "    df_path: str,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 2e-5,\n",
    "    num_epochs: int = 3,\n",
    "    max_length: int = 4096,\n",
    "    min_ref_count: int = 100,\n",
    "    output_dir: str = \"./longformer_platform_trainer\",\n",
    "    chunk_chars: int = 4000, \n",
    "):\n",
    "    \"\"\"\n",
    "    df_path: news_merged.xlsx ê°™ì€ íŒŒì¼ ê²½ë¡œ\n",
    "    return: (model, tokenizer, referrers, trainer)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 4-1. ë°ì´í„° ë¡œë“œ ----------\n",
    "    # ---------- 4-1. ë°ì´í„° ë¡œë“œ ----------\n",
    "    df = pd.read_excel(df_path)\n",
    "\n",
    "    needed_cols = [\"article_id\", \"content\", \"views_total\", \"referrer\", \"share\"]\n",
    "    for c in needed_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ {c} ì´(ê°€) ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ---------- 4-1-1. referrer ê°€ê³µ (ë³µì‚¬í•´ì„œ ì‚¬ìš©) ----------\n",
    "    # ì›ë³¸ ë³´í˜¸ìš© (ì—‘ì…€ì€ ê±´ë“œë¦¬ì§€ ì•Šê³ , ë©”ëª¨ë¦¬ ìƒ dfë§Œ ë³€ê²½)\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Facebook, namu.wiki, ì•¼í›„ â†’ 'ê¸°íƒ€'ë¡œ ê·¸ë£¹í™”í•œ ìƒˆ ì»¬ëŸ¼ ë§Œë“¤ê¸°\n",
    "    df[\"referrer_grouped\"] = df[\"referrer\"].replace({\n",
    "        \"Facebook\": \"ê¸°íƒ€\",\n",
    "        \"namu.wiki\": \"ê¸°íƒ€\",\n",
    "        \"ì•¼í›„\": \"ê¸°íƒ€\",\n",
    "    })\n",
    "\n",
    "    # 2) referrer_grouped ê¸°ì¤€ìœ¼ë¡œ ê°œìˆ˜ ì¤„ì´ê¸°\n",
    "    #    ë„¤ì´ë²„: 10000, ë„¤ì´ë²„ ë¸”ë¡œê·¸: 7000, Google: 5000\n",
    "    max_per_class = {\n",
    "        \"ë„¤ì´ë²„\": 10000,\n",
    "        \"ë„¤ì´ë²„ ë¸”ë¡œê·¸\": 7000,\n",
    "        \"Google\": 5000,\n",
    "        # ê¸°íƒ€, Daum ë“±ì€ ì œí•œ ì—†ìŒ\n",
    "    }\n",
    "\n",
    "    balanced_parts = []\n",
    "    for label, group in df.groupby(\"referrer_grouped\"):\n",
    "        if label in max_per_class and len(group) > max_per_class[label]:\n",
    "            sampled = group.sample(max_per_class[label], random_state=42)\n",
    "            balanced_parts.append(sampled)\n",
    "        else:\n",
    "            balanced_parts.append(group)\n",
    "\n",
    "    df = pd.concat(balanced_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # ì´í›„ ì½”ë“œì—ì„œëŠ” referrer_groupedë¥¼ referrerë¡œ ì‚¬ìš©í•˜ë„ë¡ êµì²´\n",
    "    df[\"referrer\"] = df[\"referrer_grouped\"]\n",
    "    df = df.drop(columns=[\"referrer_grouped\"])\n",
    "\n",
    "    # ê°€ê³µ í›„ ë¶„í¬ í™•ì¸í•´ë³´ê³  ì‹¶ìœ¼ë©´ ì´ê±° ì ê¹ ì°ì–´ë´ë„ ë¨\n",
    "    print(df[\"referrer\"].value_counts())\n",
    "\n",
    "\n",
    "    # ---------- 4-2. referrer ë“±ì¥ íšŸìˆ˜ í•„í„° ----------\n",
    "    ref_counts = df[\"referrer\"].value_counts()\n",
    "    valid_referrers = ref_counts[ref_counts >= min_ref_count].index.tolist()\n",
    "    print(\"ì‚¬ìš©í•  referrer ìˆ˜:\", len(valid_referrers))\n",
    "    print(\"ì˜ˆì‹œ referrer:\", valid_referrers[:10])\n",
    "\n",
    "    df = df[df[\"referrer\"].isin(valid_referrers)].copy()\n",
    "\n",
    "    # ---------- 4-3. share -> 0~1 ----------\n",
    "    df[\"share_frac\"] = df[\"share\"] / 100.0\n",
    "\n",
    "    # ---------- 4-4. ì¡°íšŒìˆ˜ ê°€ì¤‘ soft label ----------\n",
    "    # y(article, referrer) = sum(views * share_frac) / sum(views)\n",
    "    grouped = (\n",
    "        df\n",
    "        .groupby([\"article_id\", \"referrer\"])\n",
    "        .apply(lambda g: (g[\"views_total\"] * g[\"share_frac\"]).sum() / g[\"views_total\"].sum())\n",
    "        .reset_index(name=\"soft_label\")\n",
    "    )\n",
    "\n",
    "    # ---------- 4-5. article_id ê¸°ì¤€ referrer ë¶„í¬ pivot ----------\n",
    "    label_mat = (\n",
    "        grouped\n",
    "        .pivot(index=\"article_id\", columns=\"referrer\", values=\"soft_label\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # ì •ê·œí™” (í•© = 1)\n",
    "    row_sum = label_mat.sum(axis=1)\n",
    "    row_sum[row_sum == 0.0] = 1.0\n",
    "    label_mat = label_mat.div(row_sum, axis=0)\n",
    "\n",
    "    # ---------- 4-6. ê¸°ì‚¬ ë³¸ë¬¸ ë¶™ì´ê¸° ----------\n",
    "    article_text = (\n",
    "        df\n",
    "        .drop_duplicates(\"article_id\")\n",
    "        .set_index(\"article_id\")[[\"content\"]]\n",
    "    )\n",
    "\n",
    "    dataset_df = article_text.join(label_mat, how=\"inner\").dropna()\n",
    "\n",
    "    referrer_labels = list(label_mat.columns)\n",
    "    num_labels = len(referrer_labels)\n",
    "\n",
    "    print(\"ìµœì¢… ë°ì´í„° í¬ê¸°:\", dataset_df.shape)\n",
    "    print(\"ë¼ë²¨(referrer) ê°œìˆ˜:\", num_labels)\n",
    "\n",
    "    label_array = dataset_df[referrer_labels].values\n",
    "    eps = 1e-12\n",
    "    entropy = -(label_array * np.log(label_array + eps)).sum(axis=1).mean()\n",
    "    print(\"í‰ê·  ë¼ë²¨ ì—”íŠ¸ë¡œí”¼ H(y):\", entropy)\n",
    "\n",
    "\n",
    "    chunk_chars = 4000\n",
    "    \n",
    "    expanded_rows = []\n",
    "    for article_id, row in dataset_df.iterrows():\n",
    "        chunks = split_into_chunks(row[\"content\"], chunk_chars=chunk_chars)\n",
    "        for ci, chunk in enumerate(chunks):\n",
    "            new_row = row.copy()\n",
    "            new_row[\"content\"] = chunk\n",
    "            new_row[\"chunk_id\"] = ci\n",
    "            new_row[\"article_id\"] = article_id\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "\n",
    "    # chunk 1000 í† í° ì´í•˜ ì—†ì• ê¸°\n",
    "    filtered_rows = []\n",
    "    for row in expanded_rows:\n",
    "        tokens = tokenizer.encode(row[\"content\"], add_special_tokens=True)\n",
    "        if len(tokens) >= 1000:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "\n",
    "    dataset_df = pd.DataFrame(filtered_rows).set_index(\"article_id\")\n",
    "    print(\"chunk í™•ì¥ í›„ ë°ì´í„° í¬ê¸°:\", dataset_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- 4-7. train / val split ë¹„ìœ¨ ë™ì¼í•˜ê²Œ ë‚˜ëˆ”----------\n",
    "    dataset_df[\"major_referrer\"] = dataset_df[referrer_labels].idxmax(axis=1)\n",
    "\n",
    "    # 2) Stratified split\n",
    "    train_df, val_df = train_test_split(\n",
    "        dataset_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset_df[\"major_referrer\"],   # â† í•µì‹¬!\n",
    "    )\n",
    "\n",
    "    # ---------- 4-8. í† í¬ë‚˜ì´ì € & Dataset ----------\n",
    "\n",
    "    train_dataset = NewsLongformerDataset(train_df, referrer_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset   = NewsLongformerDataset(val_df,   referrer_labels, tokenizer, max_length=max_length)\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- 4-9. ëª¨ë¸ ----------\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobigbird-bert-base\",\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # ---------- 4-10. TrainingArguments ----------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",  # eval_loss ê¸°ì¤€ìœ¼ë¡œ best model\n",
    "        report_to=\"none\",\n",
    "#         gradient_accumulation_steps=4,\n",
    "        save_safetensors=True,\n",
    "    )\n",
    "\n",
    "    # ---------- 4-11. Trainer ----------\n",
    "    trainer = SoftLabelTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # ---------- 4-12. í•™ìŠµ ----------\n",
    "    trainer.train()\n",
    "\n",
    "    # ---------- 4-13. ìµœì¢… í‰ê°€ ----------\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(\"\\nìµœì¢… í‰ê°€ ê²°ê³¼:\")\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "\n",
    "        # ---------- 4-14. best / last checkpoint ì •ë¦¬ ----------\n",
    "\n",
    "    # 1) best checkpoint ê²½ë¡œ (eval_loss ê¸°ì¤€)\n",
    "    best_ckpt = trainer.state.best_model_checkpoint\n",
    "    print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "    # 2) output_dir ì•„ë˜ì˜ ëª¨ë“  checkpoint-* ë””ë ‰í† ë¦¬ ëª©ë¡\n",
    "    ckpt_dirs = sorted(\n",
    "        [\n",
    "            p\n",
    "            for p in glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "            if os.path.isdir(p)\n",
    "        ],\n",
    "        key=lambda p: int(p.split(\"-\")[-1])  # \"checkpoint-1234\"ì˜ ìˆ«ì ë¶€ë¶„ ê¸°ì¤€ ì •ë ¬\n",
    "    )\n",
    "\n",
    "    last_ckpt = ckpt_dirs[-1] if ckpt_dirs else None\n",
    "    print(\"Last checkpoint:\", last_ckpt)\n",
    "\n",
    "    # 3) best ëª¨ë¸ì„ ë”°ë¡œ ì €ì¥ (best_model ë””ë ‰í† ë¦¬)\n",
    "    if best_ckpt is not None:\n",
    "        best_model_dir = os.path.join(output_dir, \"best_model\")\n",
    "        print(\"Saving best model to:\", best_model_dir)\n",
    "        # best checkpointì—ì„œ ë‹¤ì‹œ ë¡œë“œí•´ì„œ ì €ì¥\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            best_ckpt,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        best_model.save_pretrained(best_model_dir)\n",
    "        tokenizer.save_pretrained(best_model_dir)\n",
    "\n",
    "    # 4) ë§ˆì§€ë§‰ checkpoint ê¸°ì¤€ ëª¨ë¸ë„ ë”°ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ (ì„ íƒ)\n",
    "    if last_ckpt is not None:\n",
    "        last_model_dir = os.path.join(output_dir, \"last_model\")\n",
    "        print(\"Saving last model to:\", last_model_dir)\n",
    "        last_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            last_ckpt,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        last_model.save_pretrained(last_model_dir)\n",
    "        tokenizer.save_pretrained(last_model_dir)\n",
    "\n",
    "    # 5) checkpoint ë””ë ‰í† ë¦¬ëŠ” \"best + last\"ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ\n",
    "    for ckpt in ckpt_dirs:\n",
    "        if ckpt != best_ckpt and ckpt != last_ckpt:\n",
    "            print(\"Removing old checkpoint:\", ckpt)\n",
    "            shutil.rmtree(ckpt)\n",
    "\n",
    "    return trainer.model, tokenizer, referrer_labels, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8689bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referrer\n",
      "ë„¤ì´ë²„        10000\n",
      "ë„¤ì´ë²„ ë¸”ë¡œê·¸     7000\n",
      "Google      5000\n",
      "ê¸°íƒ€          4244\n",
      "Daum        2278\n",
      "AI ê²€ìƒ‰ì—”ì§„     1244\n",
      "Bing        1189\n",
      "Name: count, dtype: int64\n",
      "ì‚¬ìš©í•  referrer ìˆ˜: 7\n",
      "ì˜ˆì‹œ referrer: ['ë„¤ì´ë²„', 'ë„¤ì´ë²„ ë¸”ë¡œê·¸', 'Google', 'ê¸°íƒ€', 'Daum', 'AI ê²€ìƒ‰ì—”ì§„', 'Bing']\n",
      "ìµœì¢… ë°ì´í„° í¬ê¸°: (1618, 8)\n",
      "ë¼ë²¨(referrer) ê°œìˆ˜: 7\n",
      "í‰ê·  ë¼ë²¨ ì—”íŠ¸ë¡œí”¼ H(y): 0.9347150455321372\n",
      "chunk í™•ì¥ í›„ ë°ì´í„° í¬ê¸°: (2220, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at monologg/kobigbird-bert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/u4005/.venv/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3219298/1456304933.py:328: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SoftLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SoftLabelTrainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "df: /home/u4005/.triton/autotune: No such file or directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/bin/ld: cannot find -lcufile\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5328' max='5328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5328/5328 46:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.788100</td>\n",
       "      <td>1.758769</td>\n",
       "      <td>0.488739</td>\n",
       "      <td>1.758769</td>\n",
       "      <td>0.037456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.816900</td>\n",
       "      <td>1.732279</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>1.732279</td>\n",
       "      <td>0.036327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.627500</td>\n",
       "      <td>1.749698</td>\n",
       "      <td>0.475225</td>\n",
       "      <td>1.749698</td>\n",
       "      <td>0.037433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [444/444 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ìµœì¢… í‰ê°€ ê²°ê³¼:\n",
      "eval_loss: 1.732278823852539\n",
      "eval_accuracy: 0.4797297297297297\n",
      "eval_cross_entropy: 1.7322789430618286\n",
      "eval_mse: 0.036326758563518524\n",
      "eval_runtime: 53.4297\n",
      "eval_samples_per_second: 8.31\n",
      "eval_steps_per_second: 8.31\n",
      "epoch: 3.0\n",
      "Best checkpoint: ./longformer_3epoch_new/checkpoint-3552\n",
      "Last checkpoint: ./longformer_3epoch_new/checkpoint-5328\n",
      "Saving best model to: ./longformer_3epoch_new/best_model\n",
      "Saving last model to: ./longformer_3epoch_new/last_model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'last_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../AI_model/ì‹ ë¬¸ ë°ì´í„°/news_merged_grouped_balanced.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 54\u001b[0m     model, tokenizer, referrers, trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_ref_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./longformer_3epoch_new\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 386\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(df_path, batch_size, lr, num_epochs, max_length, min_ref_count, output_dir, chunk_chars)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving last model to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, last_model_dir)\n\u001b[1;32m    382\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    383\u001b[0m         best_ckpt,\n\u001b[1;32m    384\u001b[0m         num_labels\u001b[38;5;241m=\u001b[39mnum_labels,\n\u001b[1;32m    385\u001b[0m     )\n\u001b[0;32m--> 386\u001b[0m     \u001b[43mlast_model\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(last_model_dir)\n\u001b[1;32m    387\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(last_model_dir)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# 5) checkpoint ë””ë ‰í† ë¦¬ëŠ” \"best + last\"ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 5. ì˜ˆì¸¡ í•¨ìˆ˜ (í™•ë¥  10% ë¯¸ë§Œ ì¶œë ¥ X)\n",
    "# ===============================\n",
    "\n",
    "def predict_platform_distribution(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    referrers,\n",
    "    max_length: int = 4096,\n",
    "    threshold: float = 0.1,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        global_attention_mask = torch.zeros_like(attention_mask).to(DEVICE)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "        )\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "        \n",
    "    result = {\n",
    "        ref: float(p)\n",
    "        for ref, p in sorted(\n",
    "            zip(referrers, probs),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if p >= threshold\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. ì‚¬ìš© ì˜ˆì‹œ\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = \"../AI_model/ì‹ ë¬¸ ë°ì´í„°/news_merged_grouped_balanced.xlsx\"\n",
    "\n",
    "    model, tokenizer, referrers, trainer = train_model(\n",
    "        df_path=excel_path,\n",
    "        batch_size=1,\n",
    "        lr=1e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=4096,\n",
    "        min_ref_count=100,\n",
    "        output_dir=\"./longformer_3epoch_new\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from deepspeed)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (1.24.3)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-13.580.82-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (24.2)\n",
      "Requirement already satisfied: psutil in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (7.0.0)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (2.10.6)\n",
      "Requirement already satisfied: torch in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (2.4.1)\n",
      "Requirement already satisfied: tqdm in /home/u4005/.venv/lib/python3.8/site-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/u4005/.venv/lib/python3.8/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/u4005/.venv/lib/python3.8/site-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/u4005/.venv/lib/python3.8/site-packages (from pydantic>=2.0.0->deepspeed) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (3.16.1)\n",
      "Requirement already satisfied: sympy in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/u4005/.venv/lib/python3.8/site-packages (from torch->deepspeed) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/u4005/.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/u4005/.venv/lib/python3.8/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/u4005/.venv/lib/python3.8/site-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading msgpack-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading nvidia_ml_py-13.580.82-py3-none-any.whl (49 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763296 sha256=90738e4def1dccf7b0c9c5cc1673bd517bdd7f6333412f8f936e6f7ac065268c\n",
      "  Stored in directory: /home/u4005/.cache/pip/wheels/30/1d/94/a05d95c116aa0a299a3494cd8a2bae3d3d5f2d5ba7ea917ea5\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, ninja, msgpack, einops, deepspeed\n",
      "Successfully installed deepspeed-0.18.2 einops-0.8.1 hjson-3.1.0 msgpack-1.1.1 ninja-1.13.0 nvidia-ml-py-13.580.82 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6207f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
