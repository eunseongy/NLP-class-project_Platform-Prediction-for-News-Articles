{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b67224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_chars=4000):\n",
    "    \"\"\"\n",
    "    긴 문자열을 chunk_chars 글자 단위로 잘라 리스트로 반환.\n",
    "    ex) chunk_chars=4000이면 0~3999, 4000~7999, ... 이런 식으로 나뉨.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    return [text[i:i + chunk_chars] for i in range(0, len(text), chunk_chars)]\n",
    "\n",
    "\n",
    "def build_demographics_soft_labels(\n",
    "    demo_path1: str,\n",
    "    demo_path2: str,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    demographics_part001/002.xlsx를 사용해\n",
    "    article_id 기준 demographics soft label matrix 생성\n",
    "    return:\n",
    "      - label_mat: index = article_id, columns = demo_group (soft label 분포)\n",
    "      - demo_labels: demo_group 이름 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    df1 = pd.read_excel(demo_path1)\n",
    "    df2 = pd.read_excel(demo_path2)\n",
    "    df_demo = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df_demo = df_demo[~((df_demo[\"views\"] == 0) & (df_demo[\"ratio\"] == 0))].copy()\n",
    "\n",
    "    df_demo[\"ratio_frac\"] = df_demo[\"ratio\"] / 100.0\n",
    "\n",
    "    df_demo[\"demo_group\"] = df_demo[\"age_group\"].astype(str) + \"_\" + df_demo[\"gender\"].astype(str)\n",
    "\n",
    "    grouped = (\n",
    "        df_demo\n",
    "        .groupby([\"article_id\", \"demo_group\"])\n",
    "        .apply(lambda g: (g[\"views\"] * g[\"ratio_frac\"]).sum() / g[\"views\"].sum())\n",
    "        .reset_index(name=\"soft_label\")\n",
    "    )\n",
    "\n",
    "    label_mat = (\n",
    "        grouped\n",
    "        .pivot(index=\"article_id\", columns=\"demo_group\", values=\"soft_label\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    drop_cols = [c for c in label_mat.columns if str(c).startswith(\"전체_\")]\n",
    "    if drop_cols:\n",
    "        print(\"[Demographics] Dropping overall gender groups:\", drop_cols)\n",
    "        label_mat = label_mat.drop(columns=drop_cols)\n",
    "\n",
    "    label_mat = label_mat[label_mat.sum(axis=1) > 0].copy()\n",
    "\n",
    "    row_sum = label_mat.sum(axis=1)\n",
    "    row_sum[row_sum == 0.0] = 1.0\n",
    "    label_mat = label_mat.div(row_sum, axis=0)\n",
    "\n",
    "    demo_labels = list(label_mat.columns)\n",
    "    print(\"[Demographics] demo_labels 개수:\", len(demo_labels))\n",
    "\n",
    "    return label_mat, demo_labels\n",
    "\n",
    "\n",
    "def build_demographics_hard_labels(\n",
    "    demo_path1: str,\n",
    "    demo_path2: str,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    soft 분포를 먼저 만든 뒤,\n",
    "    각 article_id 마다 argmax demo_group을 뽑아서\n",
    "    해당 그룹만 1.0, 나머지는 0.0인 one-hot 벡터로 변환\n",
    "    \"\"\"\n",
    "\n",
    "    soft_mat, demo_labels = build_demographics_soft_labels(demo_path1, demo_path2)\n",
    "\n",
    "    values = soft_mat.values\n",
    "    major_idx = values.argmax(axis=1)\n",
    "\n",
    "    hard_values = np.zeros_like(values, dtype=np.float32)\n",
    "    hard_values[np.arange(len(major_idx)), major_idx] = 1.0\n",
    "\n",
    "    label_mat_hard = pd.DataFrame(\n",
    "        hard_values,\n",
    "        index=soft_mat.index,\n",
    "        columns=soft_mat.columns,\n",
    "    )\n",
    "\n",
    "    print(\"[Demographics-HARD] demo_labels 개수:\", len(demo_labels))\n",
    "    print(\"[Demographics-HARD] one-hot 예시:\")\n",
    "    print(label_mat_hard.head())\n",
    "\n",
    "    return label_mat_hard, demo_labels\n",
    "\n",
    "\n",
    "class NewsLongformerDataset(Dataset):\n",
    "    def __init__(self, df, referrers, demo_cols, tokenizer, max_length=4096):\n",
    "        \"\"\"\n",
    "        df: index = article_id\n",
    "            columns = ['content', ... referrer soft labels..., ... demo_cols ...]\n",
    "        referrers: 라벨로 쓸 referrer 이름 리스트\n",
    "        demo_cols: demographic one-hot 컬럼 리스트\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index()\n",
    "        self.referrers = referrers\n",
    "        self.demo_cols = demo_cols\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row[\"content\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            [row[r] for r in self.referrers],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        demo_vals = [float(row[c]) for c in self.demo_cols]\n",
    "        demo_feats = torch.tensor(demo_vals, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"demo_feats\": demo_feats,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "class DemoBigBirdReferrerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBigBird encoder CLS + demo_vector concat → referrer soft-label 예측\n",
    "    demo_vector = hard one-hot (또는 soft 분포도 가능)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name: str, demo_dim: int, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.feature_dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden_size + demo_dim, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        demo_feats=None,\n",
    "        labels=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if demo_feats is None:\n",
    "            raise ValueError(\"demo_feats (demographic one-hot) 가 필요합니다.\")\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        x = torch.cat([pooled, demo_feats], dim=-1)\n",
    "        x = self.feature_dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "\n",
    "class SoftLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if isinstance(outputs, dict):\n",
    "            logits = outputs[\"logits\"]\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        loss = -(labels * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    logits: (N, L)\n",
    "    labels: soft labels (N, L)  ← referrer 분포\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    logits = np.array(logits)\n",
    "    labels_soft = np.array(labels)\n",
    "\n",
    "    probs_pred = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    true_idx = labels_soft.argmax(axis=-1)\n",
    "\n",
    "    pred_idx = probs_pred.argmax(axis=-1)\n",
    "    top1_acc = (pred_idx == true_idx).mean()\n",
    "\n",
    "    k = 3\n",
    "    pred_rank_idx = np.argsort(-probs_pred, axis=-1)\n",
    "    topk_hit = np.any(pred_rank_idx[:, :k] == true_idx[:, None], axis=1)\n",
    "    top3_hit_rate = topk_hit.mean()\n",
    "\n",
    "    eps = 1e-12\n",
    "    ce = -(labels_soft * np.log(probs_pred + eps)).sum(axis=1).mean()\n",
    "\n",
    "    mse = ((probs_pred - labels_soft) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        \"top1_accuracy\": float(top1_acc),\n",
    "        \"top3_hit_rate\": float(top3_hit_rate),\n",
    "        \"cross_entropy\": float(ce),\n",
    "        \"rmse\": float(rmse),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    df_path: str,\n",
    "    demo_path1: str,\n",
    "    demo_path2: str,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 2e-5,\n",
    "    num_epochs: int = 3,\n",
    "    max_length: int = 4096,\n",
    "    min_ref_count: int = 100,\n",
    "    output_dir: str = \"./kobigbird_demo_hard_title_content\",\n",
    "    chunk_chars: int = 4000,\n",
    "):\n",
    "    \"\"\"\n",
    "    df_path: news_merged.xlsx 같은 파일 경로\n",
    "    demo_path1/2: demographics_part001/002.xlsx 경로\n",
    "    return: (model, tokenizer, referrers, demo_cols, trainer)\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_excel(df_path)\n",
    "\n",
    "    needed_cols = [\"article_id\", \"title\", \"content\", \"views_total\", \"referrer\", \"share\"]\n",
    "    for c in needed_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"필수 컬럼 {c} 이(가) 데이터에 없습니다.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    naver_set = {\"네이버\", \"네이버 블로그\"}\n",
    "    if \"period\" in df.columns:\n",
    "        group_is_naver_only = (\n",
    "            df.groupby([\"article_id\", \"period\"])[\"referrer\"]\n",
    "              .transform(lambda s: set(s.unique()).issubset(naver_set))\n",
    "        )\n",
    "        before_rows = len(df)\n",
    "        df = df[~group_is_naver_only].copy()\n",
    "        print(f\"[Referrer] NAVER/NAVER_BLOG only 그룹 제거: {before_rows} -> {len(df)} rows\")\n",
    "    else:\n",
    "        print(\"[Referrer] 'period' 컬럼이 없어 NAVER-only 필터는 article_id 단위로만 적용할 수 있습니다.\")\n",
    "        group_is_naver_only = (\n",
    "            df.groupby([\"article_id\"])[\"referrer\"]\n",
    "              .transform(lambda s: set(s.unique()).issubset(naver_set))\n",
    "        )\n",
    "        before_rows = len(df)\n",
    "        df = df[~group_is_naver_only].copy()\n",
    "        print(f\"[Referrer] NAVER/NAVER_BLOG only (article_id 기준) 제거: {before_rows} -> {len(df)} rows\")\n",
    "\n",
    "    df[\"base_text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"content\"].fillna(\"\")\n",
    "\n",
    "    df[\"referrer_grouped\"] = df[\"referrer\"].replace({\n",
    "        \"Facebook\": \"기타\",\n",
    "        \"namu.wiki\": \"기타\",\n",
    "        \"야후\": \"기타\",\n",
    "    })\n",
    "\n",
    "    df[\"referrer\"] = df[\"referrer_grouped\"]\n",
    "    df = df.drop(columns=[\"referrer_grouped\"])\n",
    "\n",
    "    print(\"[Referrer] 분포 (after grouping):\")\n",
    "    print(df[\"referrer\"].value_counts())\n",
    "\n",
    "    ref_counts = df[\"referrer\"].value_counts()\n",
    "    valid_referrers = ref_counts[ref_counts >= min_ref_count].index.tolist()\n",
    "    print(\"사용할 referrer 수:\", len(valid_referrers))\n",
    "    print(\"예시 referrer:\", valid_referrers[:10])\n",
    "\n",
    "    df = df[df[\"referrer\"].isin(valid_referrers)].copy()\n",
    "\n",
    "    df[\"share_frac\"] = df[\"share\"] / 100.0\n",
    "\n",
    "    grouped = (\n",
    "        df\n",
    "        .groupby([\"article_id\", \"referrer\"])\n",
    "        .apply(lambda g: (g[\"views_total\"] * g[\"share_frac\"]).sum() / g[\"views_total\"].sum())\n",
    "        .reset_index(name=\"soft_label\")\n",
    "    )\n",
    "\n",
    "    label_mat = (\n",
    "        grouped\n",
    "        .pivot(index=\"article_id\", columns=\"referrer\", values=\"soft_label\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    row_sum = label_mat.sum(axis=1)\n",
    "    row_sum[row_sum == 0.0] = 1.0\n",
    "    label_mat = label_mat.div(row_sum, axis=0)\n",
    "\n",
    "    article_text = (\n",
    "        df\n",
    "        .drop_duplicates(\"article_id\")\n",
    "        .set_index(\"article_id\")[[\"base_text\"]]\n",
    "        .rename(columns={\"base_text\": \"content\"})\n",
    "    )\n",
    "\n",
    "    dataset_df = article_text.join(label_mat, how=\"inner\").dropna()\n",
    "\n",
    "    referrer_labels = list(label_mat.columns)\n",
    "    num_labels = len(referrer_labels)\n",
    "\n",
    "    print(\"최종(텍스트+라벨) 데이터 크기:\", dataset_df.shape)\n",
    "    print(\"라벨(referrer) 개수:\", num_labels)\n",
    "\n",
    "    label_array = dataset_df[referrer_labels].values\n",
    "    eps = 1e-12\n",
    "    entropy = -(label_array * np.log(label_array + eps)).sum(axis=1).mean()\n",
    "    print(\"평균 라벨 엔트로피 H(y):\", entropy)\n",
    "\n",
    "    demo_hard_mat, demo_labels = build_demographics_hard_labels(demo_path1, demo_path2)\n",
    "\n",
    "    dataset_df = dataset_df.join(demo_hard_mat, how=\"inner\")\n",
    "\n",
    "    demo_cols = list(demo_hard_mat.columns)\n",
    "    print(\"demo_cols 개수:\", len(demo_cols))\n",
    "\n",
    "    expanded_rows = []\n",
    "    for article_id, row in dataset_df.iterrows():\n",
    "        chunks = split_into_chunks(row[\"content\"], chunk_chars=chunk_chars)\n",
    "        for ci, chunk in enumerate(chunks):\n",
    "            new_row = row.copy()\n",
    "            new_row[\"content\"] = chunk\n",
    "            new_row[\"chunk_id\"] = ci\n",
    "            new_row[\"article_id\"] = article_id\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "\n",
    "    filtered_rows = []\n",
    "    for row in expanded_rows:\n",
    "        tokens = tokenizer.encode(row[\"content\"], add_special_tokens=True)\n",
    "        if len(tokens) >= 1000:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    dataset_df = pd.DataFrame(filtered_rows).set_index(\"article_id\")\n",
    "    print(\"chunk 확장 후 데이터 크기:\", dataset_df.shape)\n",
    "\n",
    "    dataset_df[\"major_referrer\"] = dataset_df[referrer_labels].idxmax(axis=1)\n",
    "    major_ref = dataset_df[\"major_referrer\"]\n",
    "    print(\"[major_referrer 분포 (chunk 단위)]:\")\n",
    "    print(major_ref.value_counts(normalize=True))\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        dataset_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset_df[\"major_referrer\"],\n",
    "    )\n",
    "\n",
    "    train_dataset = NewsLongformerDataset(\n",
    "        train_df, referrer_labels, demo_cols, tokenizer, max_length=max_length\n",
    "    )\n",
    "    val_dataset = NewsLongformerDataset(\n",
    "        val_df, referrer_labels, demo_cols, tokenizer, max_length=max_length\n",
    "    )\n",
    "\n",
    "    model = DemoBigBirdReferrerModel(\n",
    "        encoder_name=\"monologg/kobigbird-bert-base\",\n",
    "        demo_dim=len(demo_cols),\n",
    "        num_labels=num_labels,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"top1_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        save_safetensors=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = SoftLabelTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(\"\\n최종 평가 결과 (best top-1 기준 모델):\")\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    best_ckpt = trainer.state.best_model_checkpoint\n",
    "    print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "    ckpt_dirs = sorted(\n",
    "        [\n",
    "            p\n",
    "            for p in glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "            if os.path.isdir(p)\n",
    "        ],\n",
    "        key=lambda p: int(p.split(\"-\")[-1])\n",
    "    )\n",
    "\n",
    "    last_ckpt = ckpt_dirs[-1] if ckpt_dirs else None\n",
    "    print(\"Last checkpoint:\", last_ckpt)\n",
    "\n",
    "    if best_ckpt is not None:\n",
    "        best_model_dir = os.path.join(output_dir, \"best_model\")\n",
    "        print(\"Saving best model dir to:\", best_model_dir)\n",
    "        os.makedirs(best_model_dir, exist_ok=True)\n",
    "        shutil.copytree(best_ckpt, best_model_dir, dirs_exist_ok=True)\n",
    "\n",
    "    if last_ckpt is not None:\n",
    "        last_model_dir = os.path.join(output_dir, \"last_model\")\n",
    "        print(\"Saving last model dir to:\", last_model_dir)\n",
    "        os.makedirs(last_model_dir, exist_ok=True)\n",
    "        shutil.copytree(last_ckpt, last_model_dir, dirs_exist_ok=True)\n",
    "\n",
    "    for ckpt in ckpt_dirs:\n",
    "        if ckpt != best_ckpt and ckpt != last_ckpt:\n",
    "            print(\"Removing old checkpoint:\", ckpt)\n",
    "            shutil.rmtree(ckpt)\n",
    "\n",
    "    return trainer.model, tokenizer, referrer_labels, demo_cols, trainer\n",
    "\n",
    "\n",
    "def predict_platform_distribution(\n",
    "    text: str,\n",
    "    age_group: str,\n",
    "    gender: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    referrers,\n",
    "    demo_cols,\n",
    "    max_length: int = 4096,\n",
    "    threshold: float = 0.1,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        target_demo = f\"{age_group}_{gender}\"\n",
    "        demo_vec = [1.0 if col == target_demo else 0.0 for col in demo_cols]\n",
    "        demo_tensor = torch.tensor(demo_vec, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            demo_feats=demo_tensor,\n",
    "        )\n",
    "        if isinstance(outputs, dict):\n",
    "            logits = outputs[\"logits\"]\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "        \n",
    "    result = {\n",
    "        ref: float(p)\n",
    "        for ref, p in sorted(\n",
    "            zip(referrers, probs),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if p >= threshold\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = \"../Korean_Spoken/news_merged_grouped_final_summary.xlsx\"\n",
    "    demo_path1 = \"../Korean_Spoken/demographics_part001.xlsx\"\n",
    "    demo_path2 = \"../Korean_Spoken/demographics_part002.xlsx\"\n",
    "\n",
    "    model, tokenizer, referrers, demo_cols, trainer = train_model(\n",
    "        df_path=excel_path,\n",
    "        demo_path1=demo_path1,\n",
    "        demo_path2=demo_path2,\n",
    "        batch_size=2,\n",
    "        lr=1e-5,\n",
    "        num_epochs=5,\n",
    "        max_length=4096,\n",
    "        min_ref_count=100,\n",
    "        output_dir=\"./kobigbird_demo_hard_title_content\",\n",
    "    )\n",
    "\n",
    "    example_text = \"10대 여성 소비 트렌드: 편의점과 패션의 변화에 대한 심층 분석 기사 본문 ...\"\n",
    "    dist = predict_platform_distribution(\n",
    "        text=example_text,\n",
    "        age_group=\"10대\",\n",
    "        gender=\"여\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        referrers=referrers,\n",
    "        demo_cols=demo_cols,\n",
    "        max_length=4096,\n",
    "        threshold=0.1,\n",
    "    )\n",
    "    print(\"\\n[예측 분포 (p>=0.1)]\")\n",
    "    for k, v in dist.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
