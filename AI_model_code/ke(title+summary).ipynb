{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 0. 공통 설정\n",
    "# ===============================\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "ENCODER_MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1. Dataset 정의 (title 텍스트 사용)\n",
    "# ===============================\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, referrers, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        df: index = article_id, columns = [\"title\", (optional: \"summary\"), <referrer1> ...]\n",
    "        referrers: 라벨로 쓸 referrer(또는 3-class) 이름 리스트\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index()\n",
    "        self.referrers = referrers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        #title만 이용\n",
    "        text = str(row[\"title\"])\n",
    "\n",
    "        #title + summary 쓰는 코드\n",
    "        # text = str(row[\"title\"]) + \" \" + str(row.get(\"summary\", \"\"))\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            [row[r] for r in self.referrers],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class SoftLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")  # soft label (분포)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Soft cross-entropy: - sum_k y_k log p_k\n",
    "        loss = -(labels * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "\n",
    "    true_idx = labels.argmax(axis=-1)\n",
    "    pred_idx = probs.argmax(axis=-1)\n",
    "    accuracy = (true_idx == pred_idx).mean()\n",
    "\n",
    "\n",
    "    eps = 1e-12\n",
    "    ce = -(labels * np.log(probs + eps)).sum(axis=-1).mean()\n",
    "\n",
    "\n",
    "    mse = ((probs - labels) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"cross_entropy\": float(ce),\n",
    "        \"rmse\": float(rmse),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    df_path: str,\n",
    "    batch_size: int = 4,\n",
    "    lr: float = 2e-5,\n",
    "    num_epochs: int = 5,\n",
    "    max_length: int = 512,\n",
    "    min_ref_count: int = 100,\n",
    "    output_dir: str = \"./koelectra_title_3class\",\n",
    "):\n",
    "    \"\"\"\n",
    "    df_path: title (그리고 선택적으로 summary) 컬럼 포함된 엑셀 경로\n",
    "    referrer → 3-class soft label (domestic_search / global_search / others)\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_excel(df_path)\n",
    "\n",
    "    needed_cols = [\"article_id\", \"title\", \"views_total\", \"referrer\", \"share\"]\n",
    "    for c in needed_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"필수 컬럼 {c} 이(가) 데이터에 없습니다.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"summary\" not in df.columns:\n",
    "        print(\"[경고] 'summary' 컬럼이 없습니다. title+summary 모드는 나중에 사용할 수 없습니다.\")\n",
    "\n",
    "\n",
    "    naver_set = {\"네이버\", \"네이버 블로그\"}\n",
    "    if \"period\" in df.columns:\n",
    "        group_is_naver_only = (\n",
    "            df.groupby([\"article_id\", \"period\"])[\"referrer\"]\n",
    "              .transform(lambda s: set(s.unique()).issubset(naver_set))\n",
    "        )\n",
    "        before_rows = len(df)\n",
    "        df = df[~group_is_naver_only].copy()\n",
    "        print(f\"[Referrer] NAVER/NAVER_BLOG only 그룹 제거: {before_rows} -> {len(df)} rows\")\n",
    "\n",
    "    print(\"[Referrer] 필터 전 referrer 분포:\")\n",
    "    print(df[\"referrer\"].value_counts())\n",
    "\n",
    "\n",
    "    ref_counts = df[\"referrer\"].value_counts()\n",
    "    valid_referrers = ref_counts[ref_counts >= min_ref_count].index.tolist()\n",
    "    print(\"[Referrer] 사용할 referrer 수:\", len(valid_referrers))\n",
    "    print(\"[Referrer] 예시 referrer:\", valid_referrers[:10])\n",
    "\n",
    "    df = df[df[\"referrer\"].isin(valid_referrers)].copy()\n",
    "\n",
    "    df[\"share_frac\"] = df[\"share\"] / 100.0\n",
    "\n",
    "    grouped = (\n",
    "        df\n",
    "        .groupby([\"article_id\", \"referrer\"])\n",
    "        .apply(lambda g: (g[\"views_total\"] * g[\"share_frac\"]).sum() / g[\"views_total\"].sum())\n",
    "        .reset_index(name=\"soft_label\")\n",
    "    )\n",
    "\n",
    "    label_mat = (\n",
    "        grouped\n",
    "        .pivot(index=\"article_id\", columns=\"referrer\", values=\"soft_label\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    row_sum = label_mat.sum(axis=1)\n",
    "    row_sum[row_sum == 0.0] = 1.0\n",
    "    label_mat = label_mat.div(row_sum, axis=0)\n",
    "\n",
    "    print(\"[원본 라벨 행렬 shape]:\", label_mat.shape)\n",
    "    print(\"[원본 referrer 컬럼들]:\", list(label_mat.columns))\n",
    "\n",
    "\n",
    "    group_cols = {\n",
    "        \"domestic_search\": [\"네이버\", \"네이버 블로그\", \"Daum\"],\n",
    "        \"global_search\": [\"Google\", \"Bing\", \"AI 검색엔진\"],\n",
    "        \"others\": [\"기타\"],\n",
    "    }\n",
    "\n",
    "    label_mat_3 = pd.DataFrame(index=label_mat.index)\n",
    "\n",
    "    for gname, cols in group_cols.items():\n",
    "        exist_cols = [c for c in cols if c in label_mat.columns]\n",
    "        if len(exist_cols) == 0:\n",
    "            label_mat_3[gname] = 0.0\n",
    "        else:\n",
    "            label_mat_3[gname] = label_mat[exist_cols].sum(axis=1)\n",
    "\n",
    " \n",
    "    row_sum_3 = label_mat_3.sum(axis=1)\n",
    "    row_sum_3[row_sum_3 == 0.0] = 1.0\n",
    "    label_mat_3 = label_mat_3.div(row_sum_3, axis=0)\n",
    "\n",
    "    label_mat = label_mat_3\n",
    "    referrer_labels = list(label_mat.columns)\n",
    "    num_labels = len(referrer_labels)\n",
    "\n",
    "    print(\"[3-class 라벨 행렬 shape]:\", label_mat.shape)\n",
    "    print(\"[3-class 라벨 컬럼들]:\", referrer_labels)\n",
    "\n",
    "    article_text = (\n",
    "        df\n",
    "        .drop_duplicates(\"article_id\")\n",
    "        .set_index(\"article_id\")[[\"title\"]]   # 기본: title만 사용\n",
    "    )\n",
    "\n",
    "    #title+summary를 하나의 텍스트로 쓰고 싶을 때:\n",
    "    \n",
    "    # df[\"base_text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"summary\"].fillna(\"\")\n",
    "    # article_text = (\n",
    "    #     df\n",
    "    #     .drop_duplicates(\"article_id\")\n",
    "    #     .set_index(\"article_id\")[[\"base_text\"]]\n",
    "    # )\n",
    "    #\n",
    "    # 그리고 아래 dataset_df 만들 때도 article_text와 join하면 됨\n",
    "    # Dataset에서는 row[\"base_text\"]를 쓰도록 수정 필요\n",
    "    \n",
    "    dataset_df = article_text.join(label_mat, how=\"inner\").dropna()\n",
    "\n",
    "    print(\"[최종 데이터 크기]:\", dataset_df.shape)\n",
    "    print(\"[라벨 개수(num_labels)]:\", num_labels)\n",
    "\n",
    "\n",
    "    dataset_df[\"major_referrer\"] = dataset_df[referrer_labels].idxmax(axis=1)\n",
    "    print(\"[major_referrer 분포 (3-class)]:\")\n",
    "    print(dataset_df[\"major_referrer\"].value_counts(normalize=True))\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        dataset_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset_df[\"major_referrer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL_NAME)\n",
    "\n",
    "    train_dataset = NewsDataset(\n",
    "        train_df,\n",
    "        referrer_labels,\n",
    "        tokenizer,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    val_dataset = NewsDataset(\n",
    "        val_df,\n",
    "        referrer_labels,\n",
    "        tokenizer,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        ENCODER_MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        report_to=\"none\",\n",
    "        save_safetensors=True,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    trainer = SoftLabelTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    " \n",
    "    eval_result = trainer.evaluate()\n",
    "    print(\"\\n최종 평가 결과 (3-class):\")\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    best_ckpt = trainer.state.best_model_checkpoint\n",
    "    print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "    ckpt_dirs = sorted(\n",
    "        [\n",
    "            p\n",
    "            for p in glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "            if os.path.isdir(p)\n",
    "        ],\n",
    "        key=lambda p: int(p.split(\"-\")[-1])\n",
    "    )\n",
    "\n",
    "    last_ckpt = ckpt_dirs[-1] if ckpt_dirs else None\n",
    "    print(\"Last checkpoint:\", last_ckpt)\n",
    "\n",
    "\n",
    "    if best_ckpt is not None:\n",
    "        best_model_dir = os.path.join(output_dir, \"best_model\")\n",
    "        print(\"Saving best model to:\", best_model_dir)\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            best_ckpt,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        best_model.save_pretrained(best_model_dir)\n",
    "        tokenizer.save_pretrained(best_model_dir)\n",
    "\n",
    "\n",
    "    if last_ckpt is not None:\n",
    "        last_model_dir = os.path.join(output_dir, \"last_model\")\n",
    "        print(\"Saving last model to:\", last_model_dir)\n",
    "        last_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            last_ckpt,\n",
    "            num_labels=num_labels,\n",
    "        )\n",
    "        last_model.save_pretrained(last_model_dir)\n",
    "        tokenizer.save_pretrained(last_model_dir)\n",
    "\n",
    "\n",
    "    for ckpt in ckpt_dirs:\n",
    "        if ckpt != best_ckpt and ckpt != last_ckpt:\n",
    "            print(\"Removing old checkpoint:\", ckpt)\n",
    "            shutil.rmtree(ckpt)\n",
    "\n",
    "    return trainer.model, tokenizer, referrer_labels, trainer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = \"../Korean_Spoken/news_merged_grouped_final_summary.xlsx\"\n",
    "\n",
    "    model, tokenizer, referrers, trainer = train_model(\n",
    "        df_path=excel_path,\n",
    "        batch_size=4,\n",
    "        lr=2e-5,\n",
    "        num_epochs=10,\n",
    "        max_length=512,\n",
    "        min_ref_count=100,\n",
    "        output_dir=\"./koelectra_title_3class_10epoch\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Training Finished ===\")\n",
    "    print(\"Referrer labels (3-class):\", referrers)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
