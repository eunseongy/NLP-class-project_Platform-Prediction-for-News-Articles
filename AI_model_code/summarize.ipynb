{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b732f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "[요약] 입력 파일: ../AI_model/신문 데이터/news_merged_grouped_final.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 기사 단위 unique 개수: 1729\n",
      "[토큰 길이 분석] 총 1729개 문서에 대해 토큰 길이 계산 중...\n",
      "  - 0/1729 처리 중...\n",
      "  - 1000/1729 처리 중...\n",
      "\n",
      "[토큰 길이 통계]\n",
      "  문서 수: 1729\n",
      "  최소 길이: 2\n",
      "  최대 길이: 13128\n",
      "  평균 길이: 2245.12\n",
      "  중앙값(median): 2319.00\n",
      "  25% 분위수: 1778.00\n",
      "  75% 분위수: 2795.00\n",
      "\n",
      "[요약] 필터링 후 1729개 '기사' 요약 시작...\n",
      "  - 0/1729 개 기사 처리 중...\n",
      "  - 50/1729 개 기사 처리 중...\n",
      "  - 100/1729 개 기사 처리 중...\n",
      "  - 150/1729 개 기사 처리 중...\n",
      "  - 200/1729 개 기사 처리 중...\n",
      "  - 250/1729 개 기사 처리 중...\n",
      "  - 300/1729 개 기사 처리 중...\n",
      "  - 350/1729 개 기사 처리 중...\n",
      "  - 400/1729 개 기사 처리 중...\n",
      "  - 450/1729 개 기사 처리 중...\n",
      "  - 500/1729 개 기사 처리 중...\n",
      "  - 550/1729 개 기사 처리 중...\n",
      "  - 600/1729 개 기사 처리 중...\n",
      "  - 650/1729 개 기사 처리 중...\n",
      "  - 700/1729 개 기사 처리 중...\n",
      "  - 750/1729 개 기사 처리 중...\n",
      "  - 800/1729 개 기사 처리 중...\n",
      "  - 850/1729 개 기사 처리 중...\n",
      "  - 900/1729 개 기사 처리 중...\n",
      "  - 950/1729 개 기사 처리 중...\n",
      "  - 1000/1729 개 기사 처리 중...\n",
      "  - 1050/1729 개 기사 처리 중...\n",
      "  - 1100/1729 개 기사 처리 중...\n",
      "  - 1150/1729 개 기사 처리 중...\n",
      "  - 1200/1729 개 기사 처리 중...\n",
      "  - 1250/1729 개 기사 처리 중...\n",
      "  - 1300/1729 개 기사 처리 중...\n",
      "  - 1350/1729 개 기사 처리 중...\n",
      "  - 1400/1729 개 기사 처리 중...\n",
      "  - 1450/1729 개 기사 처리 중...\n",
      "  - 1500/1729 개 기사 처리 중...\n",
      "  - 1550/1729 개 기사 처리 중...\n",
      "  - 1600/1729 개 기사 처리 중...\n",
      "  - 1650/1729 개 기사 처리 중...\n",
      "  - 1700/1729 개 기사 처리 중...\n",
      "[요약] 완료! 기사 단위 요약 결과를 'summary' 컬럼으로 ../AI_model/신문 데이터/news_merged_grouped_final_summary.xlsx 에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "SUMMARY_MODEL_NAME = \"gogamza/kobart-summarization\"   # KoBART 요약 모델\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_summarizer(model_name: str = SUMMARY_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    요약용 모델/토크나이저 로드\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        model.half()\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def analyze_and_filter_by_token_length(\n",
    "    df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    content_col: str = \"content\",\n",
    "    min_tokens: int = 300,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) 각 문서에 대해 KoBART 토크나이저로 토큰 길이를 계산\n",
    "    2) 전체 분포/평균/최솟값/최댓값/분위수 출력\n",
    "    3) 토큰 수가 min_tokens 이하인 행은 버린 뒤 필터링된 df 반환\n",
    "    \"\"\"\n",
    "    texts = df[content_col].fillna(\"\").tolist()\n",
    "    token_lengths: List[int] = []\n",
    "\n",
    "    print(f\"[토큰 길이 분석] 총 {len(texts)}개 문서에 대해 토큰 길이 계산 중...\")\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        enc = tokenizer(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            truncation=False,          # 여기서는 자르지 않고 전체 길이 측정\n",
    "            return_attention_mask=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "\n",
    "        if isinstance(input_ids[0], list):\n",
    "            input_ids = input_ids[0]\n",
    "        token_lengths.append(len(input_ids))\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  - {i}/{len(texts)} 처리 중...\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"token_len\"] = token_lengths\n",
    "\n",
    "    lengths = pd.Series(token_lengths)\n",
    "    print(\"\\n[토큰 길이 통계]\")\n",
    "    print(f\"  문서 수: {len(lengths)}\")\n",
    "    print(f\"  최소 길이: {lengths.min()}\")\n",
    "    print(f\"  최대 길이: {lengths.max()}\")\n",
    "    print(f\"  평균 길이: {lengths.mean():.2f}\")\n",
    "    print(f\"  중앙값(median): {lengths.median():.2f}\")\n",
    "    print(f\"  25% 분위수: {lengths.quantile(0.25):.2f}\")\n",
    "    print(f\"  75% 분위수: {lengths.quantile(0.75):.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def chunk_by_tokens(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 1024,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    긴 텍스트를 '토큰 개수' 기준으로 나누는 함수.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "        truncation=False,\n",
    "        return_tensors=None,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "\n",
    "    if isinstance(encoded[0], list):\n",
    "        encoded = encoded[0]\n",
    "\n",
    "    chunks: List[List[int]] = []\n",
    "    n_tokens = len(encoded)\n",
    "\n",
    "    for i in range(0, n_tokens, max_tokens):\n",
    "        chunk_ids = encoded[i:i + max_tokens]\n",
    "        chunks.append(chunk_ids)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def summarize_long_text(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    max_input_tokens: int = 1024,   # KoBART가 한 번에 받을 수 있는 최대 토큰 수\n",
    "    max_summary_length: int = 512,  # 최종 요약 최대 길이\n",
    "    num_beams: int = 2,\n",
    "    min_summary_tokens: int = 256,  # 최소 요약 토큰 수 (부분 요약 + 최종 요약 모두)\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    긴 문서를 처리하기 위한 계층적 요약 함수 (토큰 단위 chunking).\n",
    "    - total_tokens <= max_input_tokens 인 경우: 바로 generate (min_length 적용)\n",
    "    - 초과인 경우:\n",
    "        1) chunk 단위로 부분 요약 (각 chunk 요약에도 min_length 적용)\n",
    "        2) 부분 요약들을 합쳐 다시 한 번 최종 요약 (여기도 min_length 적용)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # 전체를 토큰화해서 토큰 개수 확인 (truncation=False)\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "        truncation=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    if isinstance(input_ids[0], list):\n",
    "        input_ids = input_ids[0]\n",
    "    total_tokens = len(input_ids)\n",
    "\n",
    "    use_amp = (DEVICE.type == \"cuda\")\n",
    "\n",
    "    # 1) max_input_tokens 이하면 바로 최종 요약\n",
    "    if total_tokens <= max_input_tokens:\n",
    "        enc2 = tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_tokens,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "            summary_ids = model.generate(\n",
    "                **enc2,\n",
    "                max_length=max_summary_length,\n",
    "                min_length=min_summary_tokens,   # \n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "            )\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary.strip()\n",
    "\n",
    "    token_chunks = chunk_by_tokens(\n",
    "        text=text,\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=max_input_tokens,\n",
    "    )\n",
    "\n",
    "    # 여러 chunk를 한 번에 batch로 요약 (부분 요약에도 min_length 적용)\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids_list = [torch.tensor(chunk, dtype=torch.long) for chunk in token_chunks]\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_list, batch_first=True, padding_value=pad_id\n",
    "    ).to(DEVICE)\n",
    "    attention_mask = (input_ids_padded != pad_id).long()\n",
    "\n",
    "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "        summary_ids_batch = model.generate(\n",
    "            input_ids=input_ids_padded,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_summary_length,\n",
    "            min_length=min_summary_tokens,    \n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    partial_summaries: List[str] = []\n",
    "    for sid in summary_ids_batch:\n",
    "        s = tokenizer.decode(sid, skip_special_tokens=True)\n",
    "        partial_summaries.append(s.strip())\n",
    "\n",
    "    combined = \"\\n\".join(partial_summaries)\n",
    "\n",
    "    enc3 = tokenizer(\n",
    "        combined,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "        final_ids = model.generate(\n",
    "            **enc3,\n",
    "            max_length=max_summary_length,\n",
    "            min_length=min_summary_tokens,  \n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "    final_summary = tokenizer.decode(final_ids[0], skip_special_tokens=True)\n",
    "    return final_summary.strip()\n",
    "\n",
    "\n",
    "\n",
    "def summarize_excel(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    content_col: str = \"content\",\n",
    "    summary_col: str = \"summary\",\n",
    "    max_input_tokens: int = 1024,\n",
    "    max_summary_length: int = 512,\n",
    "    min_tokens: int = 300,\n",
    "    num_beams: int = 2,\n",
    "    min_summary_tokens: int = 256,\n",
    "):\n",
    "    \"\"\"\n",
    "    엑셀 파일을 읽어서 **기사(article_id) 단위로** content_col을 요약하고\n",
    "    summary_col에 저장한 뒤 output_path로 저장.\n",
    "\n",
    "    - 동일 article_id는 content가 같으므로 한 번만 요약\n",
    "    - 그 summary를 같은 article_id의 모든 행에 채워 넣음\n",
    "    - referrer 기반 다운샘플링은 하지 않음\n",
    "    \"\"\"\n",
    "    print(f\"[요약] 입력 파일: {input_path}\")\n",
    "    df = pd.read_excel(input_path)\n",
    "\n",
    "    needed_cols = [content_col, \"article_id\"]\n",
    "    for c in needed_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"필수 컬럼 '{c}' 이(가) 엑셀에 없습니다.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    tokenizer, model = load_summarizer()\n",
    "\n",
    "\n",
    "    article_df = (\n",
    "        df[[\"article_id\", content_col]]\n",
    "        .drop_duplicates(subset=[\"article_id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"\\n[INFO] 기사 단위 unique 개수: {len(article_df)}\")\n",
    "\n",
    "\n",
    "    article_df = analyze_and_filter_by_token_length(\n",
    "        article_df,\n",
    "        tokenizer=tokenizer,\n",
    "        content_col=content_col,\n",
    "        min_tokens=min_tokens,\n",
    "    )\n",
    "\n",
    "    # # 토큰 필터링에서 살아남은 article_id만 사용\n",
    "    # valid_article_ids = set(article_df[\"article_id\"].tolist())\n",
    "    # df = df[df[\"article_id\"].isin(valid_article_ids)].reset_index(drop=True)\n",
    "\n",
    "    texts = article_df[content_col].fillna(\"\").tolist()\n",
    "    article_ids = article_df[\"article_id\"].tolist()\n",
    "    summaries: List[str] = []\n",
    "\n",
    "    print(f\"\\n[요약] 필터링 후 {len(texts)}개 '기사' 요약 시작...\")\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        summary = summarize_long_text(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_summary_length=max_summary_length,\n",
    "            num_beams=num_beams,\n",
    "            min_summary_tokens=min_summary_tokens,\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  - {i}/{len(texts)} 개 기사 처리 중...\")\n",
    "\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        summary_col: summaries,\n",
    "    })\n",
    "\n",
    "    df = df.merge(summary_df, on=\"article_id\", how=\"left\")\n",
    "\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"[요약] 완료! 기사 단위 요약 결과를 '{summary_col}' 컬럼으로 {output_path} 에 저장했습니다.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_excel = \"../AI_model/신문 데이터/news_merged_grouped_final.xlsx\"\n",
    "    summarized_excel = \"../AI_model/신문 데이터/news_merged_grouped_final_summary.xlsx\"\n",
    "\n",
    "    summarize_excel(\n",
    "        input_path=original_excel,\n",
    "        output_path=summarized_excel,\n",
    "        content_col=\"content\",   # 원본 기사 본문 컬럼명\n",
    "        summary_col=\"summary\",   # 요약 내용이 들어갈 컬럼명\n",
    "        max_input_tokens=1024,   # KoBART 최대 토큰 길이\n",
    "        max_summary_length=512,\n",
    "        min_tokens=300,          # 입력 토큰 수 <= 300인 기사는 버림\n",
    "        num_beams=2,\n",
    "        min_summary_tokens=256,  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35913001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 수: 15322\n",
      "최소 토큰: 6\n",
      "최대 토큰: 255\n",
      "평균 토큰: 107.78756037070879\n",
      "중앙값: 82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 요약된 엑셀 위치\n",
    "excel_path = \"../AI_model/신문 데이터/news_merged_grouped_balanced_summary.xlsx\"\n",
    "\n",
    "# KoBART 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-summarization\")\n",
    "\n",
    "# 엑셀 읽기\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# summary 컬럼 존재 확인\n",
    "if \"summary\" not in df.columns:\n",
    "    raise ValueError(\"엑셀에 'summary' 컬럼이 없습니다.\")\n",
    "\n",
    "summaries = df[\"summary\"].fillna(\"\").tolist()\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "for i, text in enumerate(summaries):\n",
    "    enc = tokenizer(\n",
    "        str(text),\n",
    "        add_special_tokens=True,\n",
    "        truncation=False,\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    ids = enc[\"input_ids\"]\n",
    "    if isinstance(ids[0], list):\n",
    "        ids = ids[0]\n",
    "\n",
    "    token_lengths.append(len(ids))\n",
    "\n",
    "print(\"문서 수:\", len(token_lengths))\n",
    "print(\"최소 토큰:\", min(token_lengths))\n",
    "print(\"최대 토큰:\", max(token_lengths))\n",
    "print(\"평균 토큰:\", sum(token_lengths) / len(token_lengths))\n",
    "print(\"중앙값:\", sorted(token_lengths)[len(token_lengths) // 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63b6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
